{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constracting a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as data\n",
    "import sklearn.model_selection as model_select\n",
    "import sklearn.tree as tree\n",
    "import sklearn.metrics as metrics\n",
    "DOT_FILE = 'iris-tree.dot'\n",
    "DEBUGGING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes =  ['setosa' 'versicolor' 'virginica']\n",
      "attributes =  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# load the built-in iris data set\n",
    "iris = data.load_iris()\n",
    "if ( DEBUGGING ):\n",
    "    print('classes = ', iris.target_names)\n",
    "    print('attributes = ', iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training instances = 112\n",
      "number of test instances = 38\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = model_select.train_test_split( iris.data, iris.target, random_state=0 )\n",
    "M_train = len( X_train )\n",
    "M_test = len( X_test )\n",
    "if ( DEBUGGING ):\n",
    "    print('number of training instances = ' + str( M_train ))\n",
    "    print('number of test instances = ' + str( M_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the decision tree\n",
    "clf = tree.DecisionTreeClassifier( random_state = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the tree model to the training data\n",
    "clf.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) How good is your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Count the number of correctly predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of correct predictions = 37.0 out of 38 = 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# predict the labels for the test set\n",
    "y_hat = clf.predict( X_test )\n",
    "# count the number of correctly predicted labels\n",
    "count = 0.0\n",
    "for i in range( M_test ):\n",
    "    if ( y_hat[i] == y_test[i] ):\n",
    "        count += 1\n",
    "score = ( count / M_test )\n",
    "print('number of correct predictions = {} out of {} = {}'.format( count, M_test, score ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use the scikit-learn classifier score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score =  1.0\n",
      "test score =  0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "print('training score = ', clf.score( X_train, y_train ))\n",
    "print('test score = ', clf.score( X_test, y_test ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use scikit-learn metrics package to compute the accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score =  0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "print('accuracy score = ', metrics.accuracy_score( y_test, y_hat ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Compute a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix =\n",
      "\t predicted-->\n",
      "actual:setosaversicolorvirginica\n",
      "1300\n",
      "0151\n",
      "009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix( y_test, y_hat )\n",
    "print('confusion matrix =')\n",
    "#print '%10s\\t%s' % ( ' ','predicted-->' )\n",
    "print('\\t predicted-->')\n",
    "#print '%10s\\t' % ( 'actual:' ),\n",
    "print('actual:', end='')\n",
    "for i in range( len( iris.target_names )):\n",
    "    #print '%10s\\t' % ( iris.target_names[i] ),\n",
    "    print( iris.target_names[i], end='' )\n",
    "#print '\\n',\n",
    "print()\n",
    "for i in range( len( iris.target_names )):\n",
    "    #print '%10s\\t' % ( iris.target_names[i] ),\n",
    "    for j in range( len( iris.target_names )):\n",
    "        #print '%10s\\t' % ( cm[i,j] ),\n",
    "        print(cm[i,j], end='') \n",
    "    #print '\\n',\n",
    "    print()\n",
    "# print '\\n',\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Compute precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score = tp / (tp + fp) =\n",
      "\t setosa = 1.0\n",
      "\t versicolor = 1.0\n",
      "\t virginica = 0.9\n"
     ]
    }
   ],
   "source": [
    "print('precision score = tp / (tp + fp) =')\n",
    "precision = metrics.precision_score( y_test, y_hat, average=None )\n",
    "for i in range( len( iris.target_names )):\n",
    "    print('\\t {} = {}'.format( iris.target_names[i], precision[i] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compute recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall score = tp / (tp + fn) =\n",
      "\t setosa = 1.0\n",
      "\t versicolor = 0.9375\n",
      "\t virginica = 1.0\n"
     ]
    }
   ],
   "source": [
    "print('recall score = tp / (tp + fn) =')\n",
    "recall = metrics.recall_score( y_test, y_hat, average=None )\n",
    "for i in range( len( iris.target_names )):\n",
    "    print('\\t {} = {}'.format( iris.target_names[i], recall[i] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Compute F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score = 2 * (precision * recall) / (precision + recall) =\n",
      "\t setosa = 1.0\n",
      "\t versicolor = 0.967741935483871\n",
      "\t virginica = 0.9473684210526316\n"
     ]
    }
   ],
   "source": [
    "print('f1 score = 2 * (precision * recall) / (precision + recall) =')\n",
    "f1 = metrics.f1_score( y_test, y_hat, average=None )\n",
    "for i in range( len( iris.target_names )):\n",
    "    print('\\t {} = {}'.format( iris.target_names[i], f1[i] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) What does the decision tree look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision path: \n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 0)\t1\n",
      "  (4, 1)\t1\n",
      "  (5, 0)\t1\n",
      "  (5, 1)\t1\n",
      "  (6, 0)\t1\n",
      "  (6, 1)\t1\n",
      "  (7, 0)\t1\n",
      "  (7, 1)\t1\n",
      "  (8, 0)\t1\n",
      "  (8, 1)\t1\n",
      "  (9, 0)\t1\n",
      "  (9, 1)\t1\n",
      "  (10, 0)\t1\n",
      "  (10, 1)\t1\n",
      "  (11, 0)\t1\n",
      "  (11, 1)\t1\n",
      "  (12, 0)\t1\n",
      "  :\t:\n",
      "  (143, 12)\t1\n",
      "  (144, 0)\t1\n",
      "  (144, 2)\t1\n",
      "  (144, 8)\t1\n",
      "  (144, 12)\t1\n",
      "  (145, 0)\t1\n",
      "  (145, 2)\t1\n",
      "  (145, 8)\t1\n",
      "  (145, 12)\t1\n",
      "  (146, 0)\t1\n",
      "  (146, 2)\t1\n",
      "  (146, 8)\t1\n",
      "  (146, 12)\t1\n",
      "  (147, 0)\t1\n",
      "  (147, 2)\t1\n",
      "  (147, 8)\t1\n",
      "  (147, 12)\t1\n",
      "  (148, 0)\t1\n",
      "  (148, 2)\t1\n",
      "  (148, 8)\t1\n",
      "  (148, 12)\t1\n",
      "  (149, 0)\t1\n",
      "  (149, 2)\t1\n",
      "  (149, 8)\t1\n",
      "  (149, 12)\t1\n"
     ]
    }
   ],
   "source": [
    "# what does the tree look like?\n",
    "print('decision path: ')\n",
    "print(clf.decision_path( iris.data ))                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output dot file written to:  iris-tree.dot\n"
     ]
    }
   ],
   "source": [
    "# output the tree to \"dot\" format for later visualising\n",
    "tree.export_graphviz( clf, out_file = DOT_FILE, class_names=iris.target_names, impurity=True )\n",
    "print('output dot file written to: ', DOT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
